---
title: 八股记录
date: 2023-02-15 21:08:54
tags:
---
- 机器学习/深度学习基础
- diffusion
- transformer
- CLIP

## 基础
- `inplace`直接改变内容不复制
- `axis`默认为0， 按行操作，1为按列操作
- `__call__`把类变成函数，直接调用

## 深度学习500问
### chapter 2
- 半监督学习： 部分数据被标记，应用在分类和回归，通过对已标记数据建模，对未标记数据进行预测
- 弱监督学习： 数据集标签是不可靠的（不正确，多标记，标记不充分，局部标记）。已知弱标签，训练算法讲数据映射到一组更强的标签（如分类到分割）。
- 分类算法：贝叶斯、决策树、SVM、KNN、逻辑回归、神经网络、Adaboosting
- 评价指标：召回率（=灵敏度），度量有多少个正例被分为正例


# 深度学习基础

### Batch normalization
- 机器学习假设训练数据和测试数据满足相同的分布，BN就是在深度神经网络训练过程中使每一层神经网络的输入保持相同的分布。
- covariate shift：如果实例合集（x,y）中输入值x的分布老是变，不符合独立同分布的假设，模型很难稳定的学规律。
- BN通过规范化手段，将每层神经网络任意神经元的输入值的分布强行拉回到（0，1）正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域，那么输入的小变化就会导致损失函数较大的变化，使得梯度变大，避免梯度消失的问题，加快收敛。（输入-> 神经元 -> BN -> 激活函数）

### 损失、代价、目标函数的区别
- 损失函数：定义在一个样本上，算的是一个样本的误差
- 代价函数：定义在整个训练集上，是所有样本的平均误差，也就是损失函数的平均
- 目标函数：最终需要优化的函数，等于经验风险+结构风险（代价函数+正则化项）

### 梯度消失和梯度爆炸
- 产生梯度不稳定的根本原因：前面层上的梯度是来自后面层上梯度的乘积，当存在过多层时，就会存在梯度不稳定的场景，比如梯度消失和梯度爆炸。
- 解决：选择合适的激活函数，使用batch normalization,梯度裁剪，参数初始化。

### 验证集和测试集
- 验证集用于调超参，监控模型是否异常（如过拟合）
- 测试集不参与学习过程以及参数调整过程

### 激活函数、交叉熵损失

### batch size
- 在一定范围内，batch size 越大，其确定的下降方向越准，引起训练震荡越小
- 增大batch size， 跑完一个epoch所需要的迭代次数越少，想达到同样的精度，所花费的时间增加，对参数的修正更加缓慢

## 注意力机制
- 在视觉中，使用mask来形成注意力机制，分为软注意力（可微）和强注意力
- 空间域：让神经网络看哪里。放弃用max pooling 和 average pooling，提出 spatial transformer，将图片中的空间域信息做对应的空间变化。将原始图片中的空间信息变换道另一个空间并保留关键信息。（权重在二维空间上）
- 通道域：卷积网络的每一层都有好多卷积核，每个卷积核对应一个特征通道。（权重在通道上）
Squeeze：使用全局平均池化
Excitation：学习各通道的依赖程度，并根据依赖程度对不同特征图进行调整。
- 混合域（CBAM）：提出残差注意力学习，不止把mask之后的特征张量作为下一层的输入，同时也将mask之前的特征张量作为下一层的输入。具体是推出一维的channel attention map 和 二维的 spatial attention map， 然后与 feature map 相乘

### 优化算法 (优缺点)
`极市平台`
影响收敛： batch size、learning rate 鞍点
1. 梯度下降法
- 鞍点
定义： 目标函数一阶导为0，在某个方向上是极大值，某个方向是极小值
判断鞍点：函数在一阶导数为0的海森矩阵为不定矩阵
逃出鞍点：用海森矩阵判断；随机梯度，给正确的梯度加扰动；随机初始化起点；增加偶尔的随机扰动
- 批量梯度下降（BGD）
```python
for i in range(epoch):
    grad = evaluate_grad(loss_func, param, data)
    param = param - learning_rate * grad
```
- 随机梯度下降（SGD）
随机选取一个样本计算梯度
```python
    for i in range(epoch):
        np.random.shuffle(data)
        for example in data:
            grad = evaluate_grad(loss_func, param, exemple)
            param = param -learning_rate * grad

```
- 小批量梯度下降(mini-batch GD)
介于BGD和SGD之间，每次选取多个样本
```python
for i in range(epoch):
    np.random.shuffle(data)
    for batch in get_batch(data, batch_size):
        grad = evaluate_grad(loss_func, param, batch)
        param = param - learning_rate * grad
```
2. 梯度下降算法改进
- 动量梯度下降算法（Momentum）
模拟物体运动的惯性，在一定程度上增加稳定性，从而学习的更快。融合上一步梯度和下一步梯度
- Nesterov accelerated gradient法（NAG
- Adagrad
- Adadelta
- RMSprop
- Adam：Adaptive Moment Estimation
学习率是会发生变化的。通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应学习率的方法。

3. 牛顿法

### Batch Normalization (手写BN)
一种正则化方法，将越来越偏的分布拉回到标准化的分布，使得激活函数的输入值落在激活函数比较敏感的区域，使梯度变大，加快学习的收敛速度，避免梯度消失的问题。
训练时，是计算整个batch的均值和方差，测试时是用预先计算好的均值和方差。
<image src="八股记录/bn.png">
其中y和b是可学习参数

### 卷积神经网络
CNN = input layer + conv layer + relu layer + pooling layer + FC layer
特征：局部连接、共享参数（卷积核）

# resnet & densesnet
resnet跳跃连接有效解决网络过深的时候梯度消失的问题

### 损失函数
<image src="八股记录/loss.png">
<image src="八股记录/smooth_l1.png">
L1 loss(MAE)梯度比较稳定，但在零点不可导，收敛比较慢；L2 loss(MSE)零点可导，但是对离群点（横轴较大）非常敏感（梯度大）；smooth L1 中和两者，收敛比L1快，对离群点不敏感
交叉熵损失
<image src="八股记录/entropy.png">
逻辑回归损失
<image src="八股记录/entropy.png">

[为什么分类问题中用交叉熵损失而不是平方差损失](https://mp.weixin.qq.com/s?__biz=MzA4ODUxNjUzMQ==&mid=2247497812&idx=2&sn=a89412d3f803727758d2848d4c4cdc54&chksm=902a4a88a75dc39e76c72727eb494893e4108e3c2e543151d989144cf27a455fd10fb915a538&mpshare=1&scene=1&srcid=0323TN8Afh6zKFPclQcibgi8&sharer_sharetime=1648029215231&sharer_shareid=2b3e20f48a2bd45a6e7e36b7b6821670#rd)

### Transformer
- self-attention [链接](https://zhuanlan.zhihu.com/p/338817680)
<image src="八股记录/attention.png">
除了`d_k`的平方根是为了防止内积过大
`QK_T`表示单词之间的attention强度，再经过softmax
- multi-head attention
